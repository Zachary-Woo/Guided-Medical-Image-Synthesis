# Guided Medical Image Synthesis Using ControlNet for Data Augmentation

## Overview
This repository implements and demonstrates a pipeline for synthesizing 2D medical images using ControlNet, a diffusion-based generative model. The primary goal is to explore the potential of using synthetic images, guided by structural information (like segmentation maps or edge maps), to augment medical datasets. This initial demonstration focuses on showing the end-to-end workflow and evaluating the feasibility of this approach for downstream tasks like classification or segmentation, particularly addressing data scarcity challenges in medical imaging.

## Project Objectives (Demonstration Focus)
1. **Demonstrate Controllable Medical Image Synthesis**
   Utilize pre-trained ControlNet models (e.g., conditioned on Canny edges or segmentation maps as proxies for specific anatomical structures) combined with a base Stable Diffusion model to generate synthetic medical images based on text prompts and structural guidance.
2. **Evaluate Downstream Impact on a Manageable Scale**
   Assess how including synthetic data generated by this pipeline influences a standard medical image analysis task (e.g., classification) using a well-defined, accessible dataset like MedMNIST.
3. **Establish a Feasible Workflow**
   Provide a working codebase and methodology that can be extended for future work involving fine-tuning on specific medical datasets and more complex conditioning.

## Problem Definition
Medical datasets often suffer from imbalances or scarcity, particularly for rare conditions. Acquiring and annotating medical data is resource-intensive. This project demonstrates a potential solution using controlled generative models to create synthetic data, aiming to improve the robustness and performance of downstream analysis models.

## Approach (Revised for Feasibility)
1. **Model Selection**
   - **Base Model:** Utilize a standard pre-trained Stable Diffusion model (e.g., v1.5) from Hugging Face.
   - **ControlNet:** Employ readily available, pre-trained ControlNet models (e.g., `lllyasviel/sd-controlnet-canny` or `lllyasviel/control_v11p_sd15_seg`) as *proxies* for structural conditioning. These models, while not specifically trained on medical data structures, allow demonstration of the conditional generation concept.
   *Justification:* Fine-tuning ControlNet specifically for diverse medical structures requires significant data and compute resources, exceeding the scope of this initial demonstration. Using pre-trained proxies establishes the pipeline's viability.

2. **Dataset (Focused Scope)**
   - **MedMNIST v2:** Primarily use subsets like `PathMNIST` (histopathology) or `OrganMNIST` (CT/MRI).
   *Justification:* MedMNIST provides standardized, lightweight, and accessible 2D medical imaging datasets suitable for rapid prototyping and demonstrating the core concepts without the overhead of managing large-scale datasets like BraTS or CAMELYON within the current project phase.

3. **Training Strategy**
   - **No ControlNet Fine-tuning:** Focus on using the pre-trained ControlNet models directly for generation.
   - **Downstream Model Training:** Train standard classification (e.g., ResNet) or segmentation (e.g., U-Net) models on the chosen MedMNIST subset. Compare performance when trained on real data versus real data augmented with synthetic images generated by the ControlNet pipeline.

4. **Evaluation**
   - **Image Quality (Qualitative):** Visually inspect generated images for coherence and alignment with prompts/conditioning. Quantitative metrics like FID/SSIM can be computed but may be less informative given the use of proxy ControlNets.
   - **Downstream Utility:** The primary evaluation metric is the performance difference (e.g., accuracy, Dice score) on the test set of the downstream task when training with and without synthetic data augmentation. This directly assesses the practical value of the generated images within this demonstrative context.

## Experiments (Demonstration Scope)
1. **Conditioned Generation**
   - Generate synthetic MedMNIST-like images using text prompts and conditioning inputs derived from real images (e.g., Canny edges for `sd-controlnet-canny`, processed masks for `control_v11p_sd15_seg`).
   - Document the generation process and show example outputs.

2. **Data Augmentation Impact**
   - Train a downstream model (e.g., ResNet18 on PathMNIST) using:
     - Real training data only.
     - Real training data + Synthetically generated data.
   - Compare validation/test metrics (e.g., accuracy, loss curves) between the two training scenarios.

## Project Structure
(Structure remains largely the same, reflecting the modular design)
├── data
│   └── medmnist
├── src
│   ├── preprocessing
│   ├── controlnet_training # (Primarily for model definition/loading utilities now)
│   ├── evaluation
│   └── utils
├── generated # Directory for generated images
├── evaluation_results # Directory for downstream task results/plots
├── train.py # (May be less relevant if no fine-tuning)
├── generate.py
├── evaluate.py
├── main.py
└── README.md

## Deviation from Original Proposal
This implementation deviates from the original proposal primarily in:
- **Dataset Scope:** Focusing exclusively on MedMNIST v2 for feasibility instead of larger, more complex datasets like BCI, BraTS, or CAMELYON.
- **ControlNet Training:** Utilizing pre-trained ControlNets as structural conditioning proxies instead of fine-tuning custom ControlNets on specific medical data.

*Reasoning:* These adjustments were made to ensure a functional end-to-end pipeline could be built and demonstrated within practical constraints, providing a solid foundation for future, more extensive work involving larger datasets and specialized model fine-tuning.

## Potential Challenges & Future Work
- **Proxy Limitations:** Pre-trained ControlNets (Canny, Seg) may not perfectly capture subtle medical anatomies. Future work should involve fine-tuning ControlNets on specific medical segmentation data.
- **Dataset Scale:** Demonstrating significant downstream improvement may require generating larger volumes of synthetic data and using larger base datasets.
- **Evaluation Rigor:** Future work could incorporate more sophisticated image quality metrics and clinical expert evaluation.

## Recommended Workflow and Tools
(Tools remain the same: Hugging Face Diffusers, MONAI, Weights & Biases/TensorBoard)

## References
(Keep relevant references, especially those related to MedMNIST, ControlNet, Diffusers)
1. MedMNIST v2: A Large-Scale Lightweight Benchmark for 2D and 3D Biomedical Image Classification
2. ControlNet for Conditional Image Generation (Original Paper/Repo)
3. Hugging Face Diffusers Library Documentation
4. MONAI - Medical Open Network for AI
5. Stable Diffusion and ControlNet official repositories and checkpoints

## License
This project is for educational and research purposes. Please consult the repository's LICENSE file for more details.