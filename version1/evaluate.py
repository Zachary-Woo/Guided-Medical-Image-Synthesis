#!/usr/bin/env python
"""
Evaluation script for downstream tasks using synthetic medical images.
"""

import os
import sys
import torch
import argparse
import logging
import numpy as np
import matplotlib.pyplot as plt
from torch.utils.data import DataLoader, ConcatDataset, Subset
import traceback
import json

# Import project modules
from version1.src.preprocessing.data_loader import MedicalImageDataset
from version1.src.preprocessing.transforms import MedicalImageTransforms
from version1.src.evaluation.downstream import train_and_evaluate_downstream
from version1.src.utils.visualization import plot_metrics, create_comparison_grid
from version1.src.utils.config import Config

# External libraries
from torchvision.datasets import ImageFolder
from torchvision import transforms as T # Use T alias
import medmnist # Import medmnist library
from medmnist import INFO # Import INFO for dataset details


def parse_args():
    """
    Parse command line arguments for evaluation.

    Returns:
        argparse.Namespace: Parsed arguments
    """
    parser = argparse.ArgumentParser(description="Evaluate downstream task performance")

    # Removed real_data_dir, as medmnist downloads automatically or uses root
    # parser.add_argument("--real_data_dir", type=str, required=True, help="Directory with real data")
    parser.add_argument("--config", type=str, default="version1/configs/medmnist_canny_demo.yaml",
                        help="Path to config file (specifies dataset name, etc.)")
    parser.add_argument("--synthetic_data_dir", type=str, required=True,
                        help="Directory containing the generated synthetic images")
    parser.add_argument("--output_dir", type=str,
                        help="Directory to save evaluation results")
    parser.add_argument("--medmnist_download_dir", type=str, default="./data",
                        help="Root directory to download MedMNIST data")
    parser.add_argument("--task", type=str, default="classification",
                        choices=["segmentation", "classification"],
                        help="Downstream task to evaluate")
    parser.add_argument("--batch_size", type=int, default=32,
                        help="Batch size for evaluation")
    parser.add_argument("--num_epochs", type=int, default=20,
                        help="Number of epochs for training")
    parser.add_argument("--image_size", type=int, default=128,
                        help="Image size for evaluation")
    parser.add_argument("--learning_rate", type=float, default=1e-4,
                        help="Learning rate for training")
    parser.add_argument("--synthetic_mask_folder", type=str,
                        help="Subfolder for synthetic masks (segmentation task only)")
    parser.add_argument("--seed", type=int, default=42,
                        help="Random seed")
    parser.add_argument("--debug", action="store_true",
                        help="Enable debug logging")

    return parser.parse_args()


# Removed load_and_split_real_data function


def load_synthetic_data(synthetic_data_dir, image_size, task, transform, mask_transform=None, synthetic_mask_folder=None):
    """
    Loads the synthetic dataset generated by the ControlNet pipeline.

    Attempts to load data based on the task:
    - 'classification': Checks if `synthetic_data_dir` looks like an ImageFolder structure
      (contains multiple subdirectories). If so, loads using `ImageFolder`. If not,
      attempts to load images from the root of `synthetic_data_dir` using
      `MedicalImageDataset` (assumes single class or labels are irrelevant).
    - 'segmentation': Uses `MedicalImageDataset`, assuming images are in the root
      (`image_folder="."`) and masks are in `synthetic_mask_folder` (relative to
      `synthetic_data_dir`).

    Args:
        synthetic_data_dir (str): Path to the directory containing generated synthetic images.
        image_size (int): Target size for image resizing (should match real data).
        task (str): The downstream task type ('classification' or 'segmentation').
        transform (Transform or tuple): The transform(s) to apply. Should match the
                                        transforms used for the real data.
                                        Expects a single transform for classification,
                                        or (image_transform, mask_transform) for segmentation.
        mask_transform (Transform, optional): Explicitly passed mask transform (used if transform is not tuple).
        synthetic_mask_folder (str, optional): Subfolder containing synthetic masks (needed for segmentation).

    Returns:
        Dataset or None: The loaded synthetic dataset, or None if the directory is not found
                       or loading fails.
    """
    if not os.path.isdir(synthetic_data_dir):
        logging.warning(f"Synthetic data directory not found: {synthetic_data_dir}. Returning None.")
        return None

    img_tfm = transform[0] if isinstance(transform, tuple) else transform
    mask_tfm = transform[1] if isinstance(transform, tuple) else mask_transform

    if task == "segmentation":
        if not synthetic_mask_folder:
            logging.error("Synthetic mask folder must be specified for segmentation task when loading synthetic data.")
            return None # Segmentation requires masks
        try:
            logging.info(f"Loading synthetic segmentation data from: {synthetic_data_dir} (masks: {synthetic_mask_folder})")
            # Assuming generated images are flat in synthetic_data_dir, masks in subfolder
            return MedicalImageDataset(
                data_dir=synthetic_data_dir,
                image_folder=".", # Load images from root
                mask_folder=synthetic_mask_folder,
                transform=img_tfm,
                mask_transform=mask_tfm
            )
        except Exception as e:
            logging.error(f"Failed to load synthetic segmentation data: {e}")
            return None

    elif task == "classification":
        # Check for ImageFolder structure
        try:
             subdirs = [d for d in os.listdir(synthetic_data_dir) if os.path.isdir(os.path.join(synthetic_data_dir, d))]
             is_image_folder = len(subdirs) > 0 and all(f.lower().endswith(('.png', '.jpg', '.jpeg', '.tif', '.bmp'))
                                                    for subdir in subdirs
                                                    for f in os.listdir(os.path.join(synthetic_data_dir, subdir)))
        except Exception:
             is_image_folder = False # Error listing means unlikely ImageFolder

        if is_image_folder:
            try:
                logging.info(f"Loading synthetic classification data using ImageFolder structure from: {synthetic_data_dir}")
                return ImageFolder(root=synthetic_data_dir, transform=img_tfm)
            except Exception as e:
                 logging.error(f"Failed to load synthetic data using ImageFolder: {e}")
                 return None
        else:
            # Load as flat directory using MedicalImageDataset (no masks)
            try:
                logging.info(f"Loading synthetic classification data from flat directory: {synthetic_data_dir}")
                # Note: This dataset may need label handling depending on how downstream model uses it.
                # MedicalImageDataset returns only images if no masks found/specified.
                return MedicalImageDataset(
                    data_dir=synthetic_data_dir,
                    image_folder=".", # Load images from root
                    mask_folder=None,
                    transform=img_tfm,
                    mask_transform=None
                )
            except Exception as e:
                logging.error(f"Could not load synthetic classification data from flat directory {synthetic_data_dir}: {e}")
                return None
    else:
        logging.error(f"Unknown task type '{task}' for loading synthetic data.")
        return None


def main():
    """
    Main evaluation function.
    """
    args = parse_args()

    # Load config FIRST to get defaults
    try:
        config = Config.from_yaml(args.config)
        if config is None:
            raise ValueError("Config loaded as None")
    except Exception as e:
        logging.error(f"Error loading config file {args.config}: {e}. Cannot proceed without config.")
        sys.exit(1)

    # Determine parameters, prioritizing command-line args over config file
    output_dir = args.output_dir or (config.training.output_dir + "_evaluation")
    batch_size = args.batch_size or config.data.batch_size
    num_epochs = args.num_epochs or config.training.downstream_num_epochs
    image_size = args.image_size or config.data.image_size
    learning_rate = args.learning_rate or config.training.downstream_learning_rate
    synthetic_data_dir = args.synthetic_data_dir # Required arg
    task = args.task # Required arg
    # mask_folder = args.mask_folder or config.data.mask_folder # Not needed for medmnist loading
    synthetic_mask_folder = args.synthetic_mask_folder # Keep for synthetic loading if seg
    # val_split = args.val_split or config.data.val_split # Not needed for medmnist loading
    # test_split = args.test_split or config.data.test_split # Not needed for medmnist loading
    seed = args.seed or config.training.seed
    medmnist_download_dir = args.medmnist_download_dir
    medmnist_dataset_name = config.data.dataset_name # Get MedMNIST dataset name from config

    # Use determined values from here on
    os.makedirs(output_dir, exist_ok=True)
    os.makedirs(medmnist_download_dir, exist_ok=True)

    log_file = os.path.join(output_dir, "evaluate.log")
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(levelname)s - %(message)s",
        handlers= [
            logging.StreamHandler(sys.stdout),
            logging.FileHandler(log_file)
        ]
    )
    logging.info(f"Logging to {log_file}")
    logging.info(f"Using MedMNIST dataset: {medmnist_dataset_name}")
    logging.info(f"Using parameters: task={task}, image_size={image_size}, batch_size={batch_size}, epochs={num_epochs}, lr={learning_rate}")
    logging.info(f"Synthetic data: {synthetic_data_dir}, Output: {output_dir}")

    # Set random seed
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)

    # Set device
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    logging.info(f"Using device: {device}")

    # --- Load Real Data using MedMNIST API --- #
    logging.info(f"Loading real MedMNIST data: {medmnist_dataset_name} (size={image_size})")
    try:
        DataClass = getattr(medmnist, medmnist_dataset_name)
    except AttributeError:
        logging.error(f"Invalid MedMNIST dataset name specified in config: {medmnist_dataset_name}")
        sys.exit(1)

    info = INFO[medmnist_dataset_name.lower()] # Get dataset info (task, channels, etc.)
    n_channels = info['n_channels']
    n_classes = len(info['label'])
    medmnist_task = info['task']
    logging.info(f"MedMNIST info: channels={n_channels}, classes={n_classes}, task={medmnist_task}")

    # Define transforms based on MedMNIST dataset info and desired size
    # Note: MedMNIST library returns PIL images, so ToTensor is needed
    # Normalization might need adjustment based on the specific dataset
    # We use the standard 0.5, 0.5 normalization used elsewhere
    norm_mean = [0.5] * n_channels
    norm_std = [0.5] * n_channels
    img_tfm = T.Compose([
        T.Resize((image_size, image_size)),
        T.ToTensor(),
        T.Normalize(mean=norm_mean, std=norm_std)
    ])

    # For segmentation tasks, masks need specific transforms
    mask_tfm = T.Compose([
        T.Resize((image_size, image_size), interpolation=T.InterpolationMode.NEAREST),
        T.ToTensor(),
        lambda x: (x > 0.5).float() # Binarize
    ]) if task == "segmentation" else None

    # Choose appropriate transform based on task
    # MedMNIST datasets usually require target_transform for labels if they aren't simple scalars
    # For classification, labels are usually okay, but check specific dataset docs if issues arise
    if task == "segmentation":
         # Segmentation datasets in MedMNIST might not exist or require special handling
         # This part assumes a hypothetical MedMNIST segmentation dataset
         if medmnist_dataset_name not in ["OrganAMNIST", "OrganCMNIST", "OrganSMNIST"]: # Add known seg datasets if any
              logging.warning(f"MedMNIST dataset {medmnist_dataset_name} might not be suitable for segmentation.")
         real_train_dataset = DataClass(split='train', transform=img_tfm, target_transform=mask_tfm, download=True, root=medmnist_download_dir, size=image_size)
         real_val_dataset = DataClass(split='val', transform=img_tfm, target_transform=mask_tfm, download=True, root=medmnist_download_dir, size=image_size)
         real_test_dataset = DataClass(split='test', transform=img_tfm, target_transform=mask_tfm, download=True, root=medmnist_download_dir, size=image_size)
         data_transform = (img_tfm, mask_tfm)
    elif task == "classification":
         real_train_dataset = DataClass(split='train', transform=img_tfm, download=True, root=medmnist_download_dir, size=image_size)
         real_val_dataset = DataClass(split='val', transform=img_tfm, download=True, root=medmnist_download_dir, size=image_size)
         real_test_dataset = DataClass(split='test', transform=img_tfm, download=True, root=medmnist_download_dir, size=image_size)
         data_transform = img_tfm
    else:
         logging.error(f"Task '{task}' not compatible with MedMNIST loading logic.")
         sys.exit(1)

    logging.info(f"Loaded real datasets: Train={len(real_train_dataset)}, Val={len(real_val_dataset)}, Test={len(real_test_dataset)}")

    # --- Load Synthetic Data --- #
    logging.info(f"Loading synthetic data from: {synthetic_data_dir}")
    synthetic_dataset = load_synthetic_data(
        synthetic_data_dir=synthetic_data_dir,
        image_size=image_size,
        task=task,
        transform=data_transform, # Pass appropriate transform(s)
        mask_transform=mask_tfm if task == "segmentation" else None,
        synthetic_mask_folder=synthetic_mask_folder
    )

    # --- Create Augmented Dataset and DataLoaders --- #
    if synthetic_dataset:
        logging.info(f"Synthetic dataset size: {len(synthetic_dataset)}")
        # Check if synthetic data loaded correctly (e.g., has items)
        if len(synthetic_dataset) > 0:
            # Combine real training data with synthetic data
            augmented_train_dataset = ConcatDataset([real_train_dataset, synthetic_dataset])
            logging.info(f"Augmented training dataset size: {len(augmented_train_dataset)}")
        else:
             logging.warning("Synthetic dataset loaded but is empty. Augmentation step skipped.")
             augmented_train_dataset = real_train_dataset
    else:
        logging.warning("Synthetic dataset not loaded. Running evaluation only on real data.")
        augmented_train_dataset = real_train_dataset # Fallback to real data only

    # Create dataloaders
    num_workers = min(os.cpu_count(), 4)
    real_train_dataloader = DataLoader(
        real_train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=True
    )
    augmented_train_dataloader = DataLoader(
        augmented_train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=True
    )
    val_dataloader = DataLoader(
        real_val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True
    ) if real_val_dataset else None # MedMNIST val split might be empty for some datasets

    test_dataloader = DataLoader(
        real_test_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True
    )

    # --- Run Comparison --- #
    all_real_metrics = None
    all_augmented_metrics = None

    # 1. Train and Evaluate on Real Data Only
    logging.info(f"=== Training downstream model on REAL data only ({task}) ===")
    try:
        real_metrics_run = train_and_evaluate_downstream(
            train_loader=real_train_dataloader,
            val_loader=val_dataloader,
            test_loader=test_dataloader,
            task=task,
            num_epochs=num_epochs,
            device=device,
            learning_rate=learning_rate,
            model_save_path=os.path.join(output_dir, "downstream_model_real_only.pth"),
            n_classes=n_classes if task == "classification" else None,
            n_channels=n_channels if task == "segmentation" else None
        )
        all_real_metrics = real_metrics_run
        logging.info("Finished training on real data.")
    except Exception as e:
        logging.error(f"Error during training/evaluation on real data: {e}")
        # Optionally, re-raise to stop execution: raise e

    # 2. Train and Evaluate on Augmented Data (if synthetic data loaded and valid)
    # Check if augmented dataset is different from real one
    if synthetic_dataset and len(synthetic_dataset) > 0 and len(augmented_train_dataset) > len(real_train_dataset):
        logging.info(f"=== Training downstream model on AUGMENTED data ({task}) ===")
        try:
            augmented_metrics_run = train_and_evaluate_downstream(
                train_loader=augmented_train_dataloader,
                val_loader=val_dataloader,
                test_loader=test_dataloader,
                task=task,
                num_epochs=num_epochs,
                device=device,
                learning_rate=learning_rate,
                model_save_path=os.path.join(output_dir, "downstream_model_augmented.pth"),
                n_classes=n_classes if task == "classification" else None,
                n_channels=n_channels if task == "segmentation" else None
            )
            all_augmented_metrics = augmented_metrics_run
            logging.info("Finished training on augmented data.")
        except Exception as e:
            logging.error(f"Error during training/evaluation on augmented data: {e}")
            # Optionally, re-raise: raise e
    else:
        logging.info("Skipping training on augmented data as synthetic dataset was not loaded or was empty.")

    # --- Plotting and Saving Results --- #
    logging.info("Plotting metrics...")

    if all_real_metrics:
         metric_key_train = "train_loss"
         metric_key_val = "val_dice" if task == "segmentation" else "val_acc"

         # Plot Training Loss Comparison
         plt.figure(figsize=(10, 5))
         plt.plot(all_real_metrics.get("train_loss", []), label="Real Data Train Loss") # Use .get for safety
         if all_augmented_metrics:
             plt.plot(all_augmented_metrics.get("train_loss", []), label="Augmented Data Train Loss")
         plt.xlabel("Epoch")
         plt.ylabel("Loss")
         plt.title(f"Downstream Task ({task}) Training Loss")
         plt.legend()
         plt.grid(True, linestyle="--", alpha=0.7)
         plt.savefig(os.path.join(output_dir, "comparison_training_loss.png"))
         plt.close()

         # Plot Validation Metric Comparison
         # Check if validation loader and metrics exist
         if val_dataloader and metric_key_val in all_real_metrics and all_real_metrics.get(metric_key_val):
             plt.figure(figsize=(10, 5))
             plt.plot(all_real_metrics[metric_key_val], label=f"Real Data Val {metric_key_val}")
             if all_augmented_metrics and metric_key_val in all_augmented_metrics and all_augmented_metrics.get(metric_key_val):
                 plt.plot(all_augmented_metrics[metric_key_val], label=f"Augmented Data Val {metric_key_val}")
             plt.xlabel("Epoch")
             plt.ylabel(metric_key_val)
             plt.title(f"Downstream Task ({task}) Validation Performance")
             plt.legend()
             plt.grid(True, linestyle="--", alpha=0.7)
             plt.savefig(os.path.join(output_dir, f"comparison_validation_{metric_key_val}.png"))
             plt.close()
         else:
             logging.info("Skipping validation metric plot (no validation data or metrics found).")

         # Print Final Test Metrics
         test_metric_key = "test_dice" if task == "segmentation" else "test_acc"
         final_real_test_metric = all_real_metrics.get(test_metric_key)
         final_aug_test_metric = all_augmented_metrics.get(test_metric_key) if all_augmented_metrics else None

         if final_real_test_metric is not None:
              logging.info(f"Final Test Metric ({test_metric_key}) with REAL data: {final_real_test_metric:.4f}")
         if final_aug_test_metric is not None:
              logging.info(f"Final Test Metric ({test_metric_key}) with AUGMENTED data: {final_aug_test_metric:.4f}")
              if final_real_test_metric is not None:
                   improvement = final_aug_test_metric - final_real_test_metric
                   logging.info(f"Improvement: {improvement:.4f} ({'+' if improvement > 0 else ''}{improvement/final_real_test_metric*100:.2f}%)")

    logging.info(f"Evaluation complete. Results saved to {output_dir}")


if __name__ == "__main__":
    main() 